# ---------------------------
# Tools and Libraries
# ---------------------------

import os
import pandas as pd
import joblib
import textwrap
import time
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, PolynomialFeatures
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import log_loss, f1_score, accuracy_score, classification_report

# ---------------------------
# Load and Clean Data
# ---------------------------

print("Importing data...")

script_dir = os.path.dirname(__file__) # Where current .py file is located
csv_path = os.path.join(script_dir, '..', 'data', 'pdw_dataset.csv')

df = pd.read_csv(csv_path)
def cleaner(df):
    """
    Function to clean the dataframe by converting feature columns to numeric, converting timestamps to datetime and reordering the columns.

    Parameters
    ----------
    df : pandas.DataFrame -> The dataframe to be cleaned.

    Returns
    -------
    returns a dataframe free of null values and duplicate rows and generates a generic report for each instance
        """
    
    # Variables
    clean_df = df.copy()
    clean_df["Radar_Function"] = clean_df["Radar_Function"].str.replace(' / Tracking', ' ').str.strip()
    clean_df["Radar_Function"] = clean_df["Radar_Function"].str.replace(' / Ground Mapping', ' ').str.strip()
    clean_df["Radar_Function"] = clean_df["Radar_Function"].str.replace(' / Weather', ' ').str.strip()
    feature_cols = ['RF_MHz', 'PW_us', 'PRI_us', 'Amplitude_dB']
    
    # Dtype Conversions
    for col in feature_cols:
        clean_df[col] = pd.to_numeric(clean_df[col], errors='coerce')   # Convert strings or bad values to NaN
    
    date_time = pd.to_datetime(clean_df['Timestamp'])
    clean_df['Date'] = date_time.dt.date                                # Convert to datetime
    clean_df['Time'] = date_time.dt.time                                # Convert to datetime
    clean_df['DDHHMMZ'] = date_time.dt.strftime('%d%H%MZ')              # Used in report column

    # Remove duplicate rows and missing data
    clean_df = clean_df.dropna()
    clean_df = clean_df.drop_duplicates(keep='last')

   # Generate report variables
    julian_day = datetime.now().strftime('%j')
    dt_now = datetime.now().strftime('%d%H%M')

    header = 'EXERCISE EXERCISE EXERCISE'
    classification = 'U N C L A S S I F I E D'
    op_line = 'OPER/GALVANIZE//'
    msgid = f'MSGID/FAKEREP/BC/{julian_day}//'
    clean_df['SOI'] = clean_df.apply(lambda x: (f'SOI/-/{x['DDHHMMZ']}/{dt_now}Z/SYS_NOT/EMITTER_{x['Emitter_ID']}//'), axis=1)
    clean_df['NARR'] = clean_df.apply(lambda x: (f"""NARR: ON {x['Date']} {x['Radar_Function'].upper()} SYSTEM (EMITTER_{x['Emitter_ID']}) WAS OBSERVED IVO {x['Location_MGRS']}."""), axis=1)
    clean_df['PARAMS'] = clean_df.apply(lambda x: (f"""PRMS/FREQ:{x['RF_MHz']:.4f} MHZ/PRI:{x['PRI_us']:010.3f}/PW:{x['PW_us']:010.3f}/AMP:{x['Amplitude_dB']:010.3f} DB/\nDOA:{x['DOA_deg']:.3f} DEGREES"""), axis=1)
    ampl_line = 'AMPL/AUTOGENERATED WITH ML ALGORITHMS//'
    
    # Format variables
    def char_limit(row):
        limit = textwrap.fill(row, width=69)
        return limit

    clean_df['NARR'] = clean_df['NARR'].apply(char_limit)

    # Generate report
    clean_df['Report'] = clean_df.apply(lambda x: (f"""{header}\n{classification}\n{op_line}\n{msgid}\n{x['SOI']}\n{x['NARR']}\n{x['PARAMS']}\n{ampl_line}"""), axis=1)
    clean_df['Report'] = clean_df['Report'].astype(str)

    # Reorder
    clean_df = clean_df[['Date', 'Time', 'Emitter_ID', 'Radar_Function', 'RF_MHz', 'PW_us', 'PRI_us', 'Amplitude_dB', 'DOA_deg', 'Location_MGRS', 'Report']] # Reorder and dismiss unneeded columns
    clean_df = clean_df.sort_values(by='Date').reset_index(drop=True)  # Arrange by date

    return clean_df

print("Import successful. Cleaning data...")

# Call the function
new_df = cleaner(df)

# Export new dataframe
new_csv_path = os.path.join(script_dir, '..', 'data', 'clean_pdw_data.csv')
new_df.to_csv(new_csv_path, index=False)

# ---------------------------
# Data Preparation
# ---------------------------

print("Data cleaned. Training models...")

# Model variables
feature_columns = ['RF_MHz', 'PW_us', 'PRI_us', 'Amplitude_dB']
X = new_df[feature_columns]
y = new_df['Emitter_ID']

# Classify features
numeric_features = X.select_dtypes(include=['float', 'int']).columns
categorical_features = X.select_dtypes(include='object').columns

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, 
    test_size=0.2, random_state=42)

# Adjustments for XGBoost and CatBoost models
if y.min() == 1:        
    y_train_adj = y_train - 1
    y_test_adj = y_test - 1
else:
    y_train_adj = y_train.copy()
    y_test_adj = y_test.copy()

# ---------------------------
# Pipelines
# ---------------------------

# Data Transformers   
num_xformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', MinMaxScaler())])
# PolynomialFeatures for Logistic Regression ONLY
num_xformer_poly = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ("poly", PolynomialFeatures(degree=2, include_bias=False)),
        ('scaler', MinMaxScaler())])  

cat_xformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
        ("onehot", OneHotEncoder(sparse_output=False, handle_unknown="ignore"))])
# Create pipelines for CatBoost without OneHotEncoder
cat_xformer_cb = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="constant", fill_value="missing"))])

# Preprocessors
preprocessor = ColumnTransformer(transformers=[
        ("num", num_xformer, numeric_features),
        ("cat", cat_xformer, categorical_features)], 
        remainder="drop")
preprocessor.set_output(transform="pandas") # to retain column names

preprocessor_poly = ColumnTransformer(transformers=[
        ("num", num_xformer_poly, numeric_features),
        ("cat", cat_xformer, categorical_features)], 
        remainder="drop")
preprocessor_poly.set_output(transform="pandas") # to retain column names

# Preprocessing for CatBoost
cb_preprocessor = ColumnTransformer(transformers=[
        ("num", num_xformer, numeric_features),
        ("cat", cat_xformer_cb, categorical_features)], 
        remainder="drop")
cb_preprocessor.set_output(transform="pandas") # to retain column names

# Model Pipes
logistic_pipe = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", LogisticRegression(C=1, max_iter=1000))])
    
tree_pipe = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", DecisionTreeClassifier(max_depth=None, min_samples_leaf=10, min_samples_split=2))])

forest_pipe = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", RandomForestClassifier(max_depth=12, min_samples_split=7, n_estimators=63))])

xgb_pipe = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", XGBClassifier(learning_rate=0.03228582611857882, max_depth=6, n_estimators=160))])

lgbm_pipe = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", LGBMClassifier(verbose=-1, learning_rate=0.0411264147156576, n_estimators=113, num_leaves=15))])

cat_pipe = Pipeline(steps=[
        ("preprocess", cb_preprocessor),
        ("model", CatBoostClassifier(verbose=0, iterations=422, learning_rate=0.08477908027279457, max_depth=3,
                                     allow_writing_files=False, train_dir=None))])

# Train models
all_models = {
    'Logistic Regression': logistic_pipe,
    'Decision Tree': tree_pipe,
    'Random Forest': forest_pipe,
    'XGBoost': xgb_pipe,
    'LightGBM': lgbm_pipe,
    'CatBoost': cat_pipe
}

print('Models trained. Saving and calculating accuracy...\n')

# ---------------------------
# Export Models
# ---------------------------

total_time = 0

for name, model in all_models.items(): 
    
    start_time = time.time()
    
    if model == xgb_pipe or model == lgbm_pipe:
        y_true = y_test_adj
        model.fit(X_train, y_train_adj)
    else:
        y_true = y_test
        model.fit(X_train, y_train)   
    
    y_pred = model.predict(X_test)
    f1 = f1_score(y_true, y_pred, average='weighted')
    accuracy = accuracy_score(y_true, y_pred)

    metadata = {
    'model_name': 'RER Classifier',
    'version': '1.1',
    'trained_date': datetime.now().strftime("%Y-%m-%d"),
    'author': 'Burton Clausen'}

    # 3. Update metadata
    metadata['algorithm'] = name
    metadata['f1_score_weighted'] = f1
    metadata['accuracy'] = accuracy
    metadata['train_time_sec'] = time.time() - start_time

    total_time += time.time() - start_time

    script_dir = os.path.dirname(__file__) # Where current .py file is located
    csv_path = os.path.join(script_dir, '..', 'data', 'pdw_dataset.csv')

    # Save model
    # os.makedirs('ml_models', exist_ok=True)
    filename = f'{name.lower().replace(" ", "_")}.pkl'
    model_path = os.path.join(script_dir, filename)
    
    joblib.dump({'model': model, 'metadata': metadata}, model_path)

    # 4. Print results
    print(f"Model saved as {filename}. Score: {f1 * 100:.2f}%")

print(f"\nMetadata attributed.\n{len(all_models)} models successfully exported in {total_time:.1f} seconds.")