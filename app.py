import os
import numpy as np
import pandas as pd
import time
from datetime import datetime, date
import textwrap
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
from PIL import Image
import streamlit as st
from sklearn.preprocessing import MinMaxScaler


# ---------------------------
# RADAR EMITTER RECOGNITION MODEL
# ---------------------------

# ---------------------------
# 1. Page & Resource Setup (CACHED)
# ---------------------------

# Page setup
st.set_page_config(
    page_title="FREQy PHOENIX",
    page_icon=":satellite:",
    layout="wide",
    initial_sidebar_state="expanded")

# Page customization
st.markdown("""
    <style>
    .block-container {padding-top: 3rem; padding-bottom: 3rem; padding-left: 1rem; padding-right: 13rem; }
    .main-title { background-color: #0f172a; color: #ec591e; padding: 5px; border-color: #ee5b1e; border-radius: 5px; text-align: center; font-size: 72px; font-weight: bold; }
    div[data-testid="stExpander"] > details > summary {background-color: #0f172a; color: white; border-radius: 6px; font-weight: bold; }            
    .section-header { background-color: #0f172a; color: green; padding: 8px 12px; border-radius: 6px; font-weight: 600; margin-top: 20px; }
    .stDownloadButton button { background-color: #0f172a; color: white; padding: 0px 8px; border: none; border-radius: 3px; cursor: pointer; font-size: 16px; }
    .stDownloadButton button:hover { background-color: #CCCCCC; }
    div[data-testid="stWidgetLabel"] p {color: #CCCCCC; /* Your desired color */font-weight: bold; }
    section[data-testid="stSidebar"] { background-color: #0f172a; }            
    div[data-testid="stSlider"] label p {color: #CCCCCC; font-size: 16px; font-weight: 200; margin-bottom: 15px;}
    div[data-baseweb="slider"] > div > div { color: #CCCCCC !important; background-color: #0f172a !important; }
    div[role="slider"] { background-color: #CCCCCC !important; border-color: #CCCCCC !important; }
    </style>
    """, unsafe_allow_html=True)

st.image('images/banner.png')

# Caching to prevent constant reloading
@st.cache_data
def load_and_clean_data(path):
    orig_df = pd.read_csv(path)
    df = orig_df.copy()
    
    # Format columns
    if 'Radar_Function' in df.columns:
        df["Radar_Function"] = df["Radar_Function"].str.split(' / ').str[0].str.strip()
    if 'Timestamp' in df.columns:
        date_time = pd.to_datetime(df['Timestamp'])
        df['Date'] = date_time.dt.date
        df['Time'] = date_time.dt.time
        df['DDHHMMZ'] = date_time.dt.strftime('%d%H%MZ')

    feature_cols = ['RF_MHz', 'PW_us', 'PRI_us', 'Amplitude_dB']
    for col in feature_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Clean dataframe
    df = df.dropna().drop_duplicates(keep='last')

   # Generate report variables
    julian_day = datetime.now().strftime('%j')
    dt_now = datetime.now().strftime('%d%H%M')

    header = 'EXERCISE EXERCISE EXERCISE'
    classification = 'U N C L A S S I F I E D'
    op_line = 'OPER/GALVANIZE//'
    msgid = f'MSGID/FAKEREP/BC/{julian_day}//'
    df['SOI'] = df.apply(lambda x: (f'SOI/-/{x['DDHHMMZ']}/{dt_now}Z/SYS_NOT/EMITTER_{x['Emitter_ID']}//'), axis=1)
    df['NARR'] = df.apply(lambda x: (f"""NARR: ON {x['Date']} {x['Radar_Function'].upper()} SYSTEM (EMITTER_{x['Emitter_ID']}) WAS OBSERVED IVO {x['Location_MGRS']}."""), axis=1)
    df['PARAMS'] = df.apply(lambda x: (f"""PRMS/FREQ:{x['RF_MHz']:.4f} MHZ/PRI:{x['PRI_us']:010.3f}/PW:{x['PW_us']:010.3f}/AMP:{x['Amplitude_dB']:010.3f} DB/\nDOA:{x['DOA_deg']:.3f} DEGREES"""), axis=1)
    ampl_line = 'AMPL/AUTOGENERATED WITH ML ALGORITHMS//'

    def char_limit(row):
        limit = textwrap.fill(row, width=69)
        return limit

    df['NARR'] = df['NARR'].apply(char_limit)
    
    # Generate report
    df['Report'] = df.apply(lambda x: (f"""{header}\n{classification}\n{op_line}\n{msgid}\n{x['SOI']}\n{x['NARR']}\n{x['PARAMS']}\n{ampl_line}"""), axis=1)
    df['Report'] = df['Report'].astype(str)

    # Reorder
    df = df[['Date', 'Time', 'Emitter_ID', 'Radar_Function', 'RF_MHz', 'PW_us', 'PRI_us', 'Amplitude_dB', 'DOA_deg', 'Location_MGRS', 'Report']] # Reorder and dismiss unneeded columns
    df = df.sort_values(by='Date').reset_index(drop=True)  # Arrange by date
    
    return orig_df, df

@st.cache_resource
def load_models():
    base_path = 'ml_models/'
    # Map the display name to the actual filename
    model_files = {
        "Logistic Regression": 'logistic_regression.pkl',
        "Decision Tree": 'decision_tree.pkl',
        "Random Forest": 'random_forest.pkl',
        "XG Boosting": 'xgboost.pkl',
        "LightGBM": 'lightgbm.pkl',
        "CatBoost": 'catboost.pkl'}
    
    model_description = {
        "Logistic Regression": "estimates the probability of an outcome based on a linear combination of input features. Fast training and miniscule file size with a slight cost in accuracy.",
        "Decision Tree": "partitions data with a flowchart to assign class labels or values. Solid accuracy, blazing speed and small file size make it a solid choice.",
        "Random Forest": "improves accuracy using many Decision Trees, each trained on a random subset of the data. Amazing accuracy with a moderate training speed. File size is much larger.",
        "XG Boosting": "strengthens predictions by training weaker models and correcting their errors. Decent accuracy, moderate speed and large file size.",
        "LightGBM": "Gradient Boosting that uses techniques which handle large datasets more efficiently. Faster and smaller than XG Boost without a cost in accuracy.",
        "CatBoost": "high-accuracy handling of categorical data to reduce overfitting compared to traditional boosting algorithms. Great accuracy, relatively small file size but slow to train."
    }

    loaded_models = {}
    
    for display_name, file_name in model_files.items():
        full_path = os.path.join(base_path, file_name)    
        model_data = joblib.load(full_path)
        file_size = os.path.getsize(full_path) / 1024
        model_data['metadata']['file_size_kb'] = file_size
        if display_name in model_description:
            model_data['metadata']['description'] = model_description[display_name]
        
        loaded_models[display_name] = model_data
        
    return loaded_models


# ---------------------------
# 2. Data & Model Loading
# ---------------------------

script_dir = os.path.dirname(__file__)
csv_path = os.path.join(script_dir, 'data', 'pdw_dataset.csv')

orig_df = load_and_clean_data(csv_path)[0]
clean_df = load_and_clean_data(csv_path)[1]

all_models_dict = load_models()


# ---------------------------
# 3. Tables, Charts, & Graphs
# ---------------------------

# Purpose
with st.expander("Project Overview"):
    st.write("**WHAT EXACTLY IS FREQy PHOENIX?**: A Radio Detection and Ranging (RADAR) Emitter Recognition application that utilizes six different machine learning algorithms to identify systems based on their parameters. FREQy PHOENIX recommends algorithms based on trained data and exports formatted reports based on user settings.")
    st.write("")
    st.write("This project is a culmination of skills acquired in the GalvanizeData & Development Immersive program. The goal is to create a model that can accurately predict an output based on provided numerical and/or categorical inputs and identify where automation can be applied to common tasks.")

# Data Preview
with st.expander("Data"):
    st.write("**IMPORTANT**: The source data utilized in this project is completely **synthesized**. It contains mimicked system external scientific parameters available from unclassified sources which is not attributable to any specific radio detection and ranging systems. Timestamps, system functions, and locational data were applied randomly. The dataset is not indicative of any collection platform capability or organizational focus. The machine learning models serve only as a showcase of programming techniques to automate classification of data for analyis.")
    st.dataframe(orig_df.iloc[[1, 850, 4289, 5892, 7673, 9923, 11887]]) # Display 8 specific rows. These were chosen to demonstrate the original data had missing values.
    df_size = f"The original dataset contains a total of {orig_df.shape[0]} rows and {orig_df.shape[1]} columns."
    st.markdown(f"<p style='text-align: right;'>{df_size}</p>", unsafe_allow_html=True)

# Exploratory Data Analysis
with st.expander("Exploratory Data Analysis"):
    col1, col2 = st.columns((5, 3))

    with col1:
        fig1, ax1 = plt.subplots(figsize=(5, 3))
        sns.countplot(data=clean_df, y='Radar_Function', order=clean_df['Radar_Function'].value_counts().index, palette='viridis', ax=ax1)
        ax1.set_title('Radar Observations by Function')
        ax1.set_xlabel('')
        ax1.set_ylabel('')
        st.pyplot(fig1)

    with col2:
        fig2, ax2 = plt.subplots(figsize=(3, 2.5))
        sns.heatmap(clean_df[['RF_MHz', 'PW_us', 'PRI_us', 'Amplitude_dB']].corr(), annot=True, cmap='coolwarm', ax=ax2)
        ax2.set_title('Feature Correlation')
        st.pyplot(fig2)


# ---------------------------
# 4. Model Comparisons
# ---------------------------
with st.expander("Machine Learning"):
    scaled_data = []
    for name, data in all_models_dict.items():
        scaled_data.append({
            "Time": data['metadata'].get('train_time_sec', 0),
            "Size": data['metadata'].get('file_size_kb', 0)})

    col1, col2 = st.columns((2))

    # Comparison Table
    with col1:
        results = []
        for name, data in all_models_dict.items():
            results.append({
                "Model": name,
                "Accuracy": data['metadata'].get('accuracy', 0)*100,
                "Train Time (s)": data['metadata'].get('train_time_sec', 0),
                "File Size (kb)": data['metadata'].get('file_size_kb', 0)})

        results_df = pd.DataFrame(results).set_index("Model")

        results_df['Scaled Time'] = MinMaxScaler().fit_transform(results_df[['Train Time (s)']])
        results_df['Scaled Size'] = MinMaxScaler().fit_transform(results_df[['File Size (kb)']])
        results_df['Overall'] = results_df.apply(lambda x: (((x['Accuracy']*.28) + ((1 -x['Scaled Time'])*1.5) + ((1 - x['Scaled Size'])*.5)) / 3), axis=1)

        comparison_df = results_df[['Accuracy', 'Train Time (s)', 'File Size (kb)', 'Overall']]
        comparison_df['Accuracy'] = comparison_df['Accuracy'].map("{:.2f}%".format)
        comparison_df['Train Time (s)'] = comparison_df['Train Time (s)'].map("{:.2f}".format)
        comparison_df['File Size (kb)'] = comparison_df['File Size (kb)'].map("{:.2f}".format)
        comparison_df['Overall'] = comparison_df['Overall'].map("{:.1f}".format)

        styled_comp_df = comparison_df.style \
            .set_properties(**{'text-align': 'center'}) \
            .highlight_max(axis=0, subset=['Accuracy', 'File Size (kb)', 'Overall'], color='lightgreen') \
            .highlight_min(axis=0, subset=['Train Time (s)'], color='lightgreen')

        st.markdown(styled_comp_df.to_html(), unsafe_allow_html=True)

    # Ranking
    with col2:
        winner = comparison_df['Overall'].idxmax()
        runner_up = comparison_df['Overall'].sort_values(ascending=False).index[1]
        honorable_mentions = comparison_df['Accuracy'].sort_values(ascending=False).index[0]
        st.write(" ")

        st.header(f"**üèÜ Winner**: {winner}")
        st.write(f"{winner} {all_models_dict[winner]['metadata']['description']}")
        
        st.header(f"**ü•à Runner Up**: {runner_up}")
        st.write(f"{runner_up} {all_models_dict[runner_up]['metadata']['description']}")


    # Feature Importance
    st.markdown('<div class="section-header"> Feature Importance / Coefficients</div>', unsafe_allow_html=True)

    model_1 = all_models_dict[winner]['model']
    model_2 = all_models_dict[runner_up]['model']
    model_3 = all_models_dict[honorable_mentions]['model']
    feature_cols = ['RF_MHz', 'PW_us', 'PRI_us', 'Amplitude_dB']

    col1, col2, col3, col4, col5 = st.columns((1.25, 1.75, 1.5, 1.25, 1.65))

    with col1:
        st.subheader(winner)
        if hasattr(model_1.named_steps['model'], "feature_importances_"):
            importances = model_1.named_steps['model'].feature_importances_
            fi_df = pd.DataFrame({"Feature": feature_cols, "Importance": importances*100})
            fi_df = fi_df.sort_values("Importance", ascending=False).set_index("Feature")
            st.dataframe(fi_df, width=200, height=178)        
        else:
            poly_feature_names = model_1.named_steps['preprocess'].get_feature_names_out()
            coefs = model_1.named_steps['model'].coef_.flatten()
            if coefs.shape[0] > 1:
                coefs_abs = np.sum(np.abs(coefs), axis=0)
            else:
                coefs_abs = np.abs(coefs.flatten())
            coef_df = pd.DataFrame({"Feature": poly_feature_names, "Importance": coefs_abs})
            coef_df = coef_df.sort_values("Importance", ascending=False).set_index("Feature")
            st.dataframe(coef_df, width=200, height=178)

    with col2:
        if hasattr(model_1.named_steps['model'], "feature_importances_"):
            importances = model_1.named_steps['model'].feature_importances_
            fi_df = pd.DataFrame({"Feature": feature_cols, "Importance": importances*100})
            fi_df = fi_df.set_index("Feature")
            fig, ax = plt.subplots(figsize=(3, 1.78))
            ax.pie(fi_df['Importance'], labels=fi_df.index, autopct='%1.1f%%', startangle=90, textprops={'fontsize': 5})
            st.pyplot(fig)
        else:
            poly_feature_names = model_1.named_steps['preprocess'].get_feature_names_out()
            coefs = model_1.named_steps['model'].coef_.flatten()
            if coefs.shape[0] > 1:
                coefs_abs = np.sum(np.abs(coefs), axis=0)
            else:
                coefs_abs = np.abs(coefs.flatten())
            coef_df = pd.DataFrame({"Feature": poly_feature_names, "Importance": coefs_abs})
            coef_df = coef_df.set_index("Feature")
            fig, ax = plt.subplots(figsize=(3, 1.78), dpi=200)
            ax.pie(coef_df['Importance'], labels=coef_df.index, autopct='%1.1f%%', startangle=90, textprops={'fontsize': 5})
            st.pyplot(fig)

    with col3:
        st.markdown(f"<br><br><br><p style='text-align: center; font-size: 72px;'>- VS -</p>", unsafe_allow_html=True)

    with col4:
        st.subheader(runner_up)
        if hasattr(model_2.named_steps['model'], "feature_importances_"):
            importances = model_2.named_steps['model'].feature_importances_
            fi_df = pd.DataFrame({"Feature": feature_cols, "Importance": importances*100})
            fi_df = fi_df.sort_values("Importance", ascending=False).set_index("Feature")
            st.dataframe(fi_df, width=200, height=178)
        else:
            poly_feature_names = model_2.named_steps['preprocess'].get_feature_names_out()
            coefs = model_2.named_steps['model'].coef_.flatten()
            if coefs.shape[0] > 1:
                coefs_abs = np.sum(np.abs(coefs), axis=0)
            else:
                coefs_abs = np.abs(coefs.flatten())
            coef_df = pd.DataFrame({"Feature": poly_feature_names, "Importance": coefs_abs})
            coef_df = coef_df.sort_values("Importance", ascending=False).set_index("Feature")
            st.dataframe(coef_df)

    with col5:
        if hasattr(model_2.named_steps['model'], "feature_importances_"):
            importances = model_2.named_steps['model'].feature_importances_
            fi_df = pd.DataFrame({"Feature": feature_cols, "Importance": importances*100})
            fi_df = fi_df.sort_values("Importance", ascending=False).set_index("Feature")
            fig, ax = plt.subplots(figsize=(3, 1.78))
            ax.pie(fi_df['Importance'], labels=fi_df.index, autopct='%1.1f%%', startangle=90, textprops={'fontsize': 5})
            st.pyplot(fig)
        else:
            poly_feature_names = model_2.named_steps['preprocess'].get_feature_names_out()
            coefs = model_2.named_steps['model'].coef_.flatten()
            if coefs.shape[0] > 1:
                coefs_abs = np.sum(np.abs(coefs), axis=0)
            else:
                coefs_abs = np.abs(coefs.flatten())
            coef_df = pd.DataFrame({"Feature": poly_feature_names, "Importance": coefs_abs})
            coef_df = coef_df.set_index("Feature")
            fig, ax = plt.subplots(figsize=(3, 1.78))
            ax.pie(coef_df['Importance'], labels=coef_df.index, autopct='%1.1f%%', startangle=90, textprops={'fontsize': 5})
            st.pyplot(fig)


# ---------------------------
# 5. Interactive Sidebar & Prediction
# ---------------------------

with st.sidebar:
    st.write("")
    st.image('images/app_logo.png', width=250)
    st.markdown('<p style="color: #CCCCCC; margin-bottom: 5px;"><br><br><br>Select Model</p>', unsafe_allow_html=True)
    selected_model_name = st.selectbox("",list(all_models_dict.keys()), label_visibility='collapsed')
    current_model_obj = all_models_dict[selected_model_name]['model']
    st.write("")

# Dynamic sliders based on dataset limits
    with st.expander("Parameter Sliders"):
        st.markdown('<p style="color: #CCCCCC; margin-bottom: 5px;">RF (MHz)</p>', unsafe_allow_html=True)
        rf = st.slider('RF (MHz)', float(clean_df['RF_MHz'].min()), float(clean_df['RF_MHz'].max()), label_visibility='collapsed')
        st.markdown('<p style="color: #CCCCCC; margin-bottom: 5px;">Pulse Width</p>', unsafe_allow_html=True)
        pw = st.slider("PW (us)", float(clean_df['PW_us'].min()), float(clean_df['PW_us'].max()), label_visibility='collapsed')
        st.markdown('<p style="color: #CCCCCC; margin-bottom: 5px;">Repetition Interval</p>', unsafe_allow_html=True)
        pri = st.slider("PRI (us)", float(clean_df['PRI_us'].min()), float(clean_df['PRI_us'].max()), label_visibility='collapsed')
        st.markdown('<p style="color: #CCCCCC; margin-bottom: 5px;">Amplitude</p>', unsafe_allow_html=True)
        amp = st.slider("Amplitude (dB)", float(clean_df['Amplitude_dB'].min()), float(clean_df['Amplitude_dB'].max()), label_visibility='collapsed')
    st.write("")

    with st.expander("Report Filters"):
        st.markdown('<br>', unsafe_allow_html=True)

        start_date, end_date = st.slider("Select date range", min_value=clean_df['Date'].min(), max_value=clean_df['Date'].max(), value=(clean_df['Date'].min(), clean_df['Date'].max()), format="YYYY-MM-DD")

        st.markdown('<br><p style="color: #CCCCCC; margin-bottom: 5px;">Select Systems</p>', unsafe_allow_html=True)
        if "checked_items" not in st.session_state:
            st.session_state.checked_items = []

        options = clean_df['Radar_Function'].unique()

        # 2. Create checkboxes and update the list
        for option in options:
            # We use the option name as a key to keep track of individual states
            if st.checkbox(option, key=f"cb_{option}"):
                if option not in st.session_state.checked_items:
                    st.session_state.checked_items.append(option)
            else:
                # Optional: Remove if unchecked
                if option in st.session_state.checked_items:
                    st.session_state.checked_items.remove(option)
        
# Prediction Input
input_data = pd.DataFrame({'RF_MHz': [rf], 'PW_us': [pw], 'PRI_us': [pri], 'Amplitude_dB': [amp]})

# Execute Prediction
prediction = current_model_obj.predict(input_data)[0]
prediction_value = prediction.item()

# Use predict_proba safely (some models may not support it, though yours do)
try:
    prob = np.max(current_model_obj.predict_proba(input_data)) * 100
    prob_text = f"with {prob:.2f}% confidence"
except:
    prob_text = ""

with st.expander("Live Predictions"):
    col1, col2, col3, col4 = st.columns((1.5, .6, 1, 3.25))
    with col1:
        st.markdown(f"<p style='text-align: right; margin-top: 50px; margin-bottom: 5px; font-size: 32px; font-weight: bold;'>Based on your input:</p>", unsafe_allow_html=True)    

    with col2:
        st.markdown(f"<p style='text-align: left; margin-top: 5px; margin-bottom: 0px; font-size: 84px; font-weight: bold;'>üì°</p>", unsafe_allow_html=True)

    with col3:  
        st.markdown(f"<p style='text-align: left; margin-top: 25px; margin-bottom: 0px; font-size: 1px; font-weight: bold'/p>", unsafe_allow_html=True)
        st.metric(label=f"{selected_model_name} estimates", value=f"Emitter {prediction}", delta=prob_text)

    with col4:
        EMITTER_FUNCTION_MAP = {
        1: 'Medium-Range Surveillance',
        2: 'Short-Range Fire Control/Tracking',
        3: 'Naval/Ground Surveillance',
        4: 'Target Acquisition/Tracking',
        5: 'Air Traffic Control',
        6: 'Early Warning', 
        7: 'Target Illumination', 
        8: 'Medium-Range Search/Ground Mapping', 
        9: 'Coastal Defense', 
        10: 'Long-Range Tracking/Weather', 
    }
        st.markdown(f"<p style='text-align: left; margin-top: 45px; margin-bottom: 0px; font-size: 38px; font-weight: bold;'>  {EMITTER_FUNCTION_MAP[prediction_value]}</p>", unsafe_allow_html=True)

with st.expander("Automated Reporting"):

    st.write(f"The following report was generated automatically during the algorithm training process. Format has been created from open source. A total of {clean_df.shape[0]} reports were generated.")

    clean_df['Probability'] = np.round(np.max(current_model_obj.predict_proba(clean_df[feature_cols]), axis=1) * 100, 2)
    clean_df['Probability'] = clean_df['Probability'].apply(lambda x: f"{x:.2f}%" if x < 100 else f"{x:.0f}%")
    clean_df['Report'] = clean_df.apply(lambda x: x['Report'].replace('AUTOGENERATED WITH ML ALGORITHMS', f'AUTOGENERATED WITH {x["Probability"]} CONFIDENCE'), axis=1)

    filtered_df = clean_df[clean_df['Radar_Function'].isin(st.session_state.checked_items)]
    filtered_df = filtered_df[(filtered_df["Date"] >= start_date) & (filtered_df["Date"] <= end_date)]


    col1, col2 = st.columns((6, 4))
    with col1:
        st.subheader("**We went from this...**")
        st.dataframe(orig_df.iloc[[1, 850, 4289, 5892, 7673, 9923, 11887]])
        df_size = f"The cleaned dataset contains a total of {clean_df.shape[0]} rows and {clean_df.shape[1]} columns."
        st.markdown(f"<p style='text-align: left;'>{df_size}</p>", unsafe_allow_html=True)

    with col2:
        st.subheader("**...to this!**")
        st.code(clean_df['Report'].iloc[np.random.randint(0, clean_df.shape[0])], language='text')
        
        col3, col4 = st.columns((3, 1.5))
        with col3:
            st.markdown(f"<p style='text-align: right; margin-top: 7px; font-size: 15px; font-weight: bold;'>Select preferred model prior to downloading >>></p>", unsafe_allow_html=True)
            
        with col4:
            st.download_button(
            label="Download Reports",
            data = filtered_df['Report'].to_csv(index=False),
            #data = clean_df.to_csv(index=False),
            file_name=f"{date.today().strftime('%Y-%m-%d')}_{selected_model_name.lower().replace(' ', '_')}_reports.csv",
            mime="text/csv")